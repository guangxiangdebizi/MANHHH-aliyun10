"""
é‡åŒ–æ„å›¾æ£€æµ‹æ¨¡å—ï¼š
- æä¾›åŸºäºŽå…³é”®è¯çš„å¿«é€Ÿåˆ¤æ–­ _detect_quant_intent
- æä¾›åŸºäºŽ Oversee åˆ¤åˆ«LLMï¼ˆGLM-4.5-flashï¼‰çš„ä¸¥æ ¼åˆ¤æ–­ is_quant_by_oversee

æ³¨æ„ï¼šåˆ¤åˆ«LLMä»…æŽ¥æ”¶â€œæœ¬æ¬¡ç”¨æˆ·æ–‡æœ¬â€ï¼Œä¸æºå¸¦ä¸Šä¸‹æ–‡
"""

import os
from typing import Optional

from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage


def _is_truthy(val: str) -> bool:
    try:
        return str(val).strip().lower() in {"1", "true", "yes", "on", "y"}
    except Exception:
        return False


def detect_quant_intent_by_keywords(raw_text: str) -> bool:
    """å·²å¼ƒç”¨ï¼šä¿ç•™ç©ºå®žçŽ°ä»¥å…¼å®¹æ—§å¼•ç”¨ï¼Œä¸å†ä½¿ç”¨å…³é”®è¯å›žé€€ã€‚"""
    return False


def _get_oversee_config() -> dict:
    try:
        # å°è¯•åŠ è½½ .envï¼ˆä¸è¦†ç›–ç³»ç»Ÿå˜é‡ï¼‰
        try:
            load_dotenv(find_dotenv(), override=False)
        except Exception:
            pass
        enabled = _is_truthy(os.getenv("OVERSEE_LLM_ENABLED", "true"))
        api_key = os.getenv("Oversee_LLM_APIKEY") or os.getenv("OVERSEE_LLM_APIKEY") or os.getenv("OVERSEE_LLM_API_KEY")
        base_url = os.getenv("OVERSEE_LLM_BASE_URL", "").strip()
        model = os.getenv("OVERSEE_LLM_MODEL", "glm-4.5-flash").strip()
        temperature = float(os.getenv("OVERSEE_LLM_TEMPERATURE", "0.1"))
        timeout = int(os.getenv("OVERSEE_LLM_TIMEOUT", "10"))
        cfg = {
            "enabled": enabled,
            "api_key": (api_key or "").strip(),
            "base_url": base_url,
            "model": model,
            "temperature": temperature,
            "timeout": timeout,
        }
        # DEBUG: æ‰“å°é…ç½®æ¦‚è§ˆï¼ˆä¸æ³„éœ²å®Œæ•´Keyï¼‰
        try:
            if _is_truthy(os.getenv("OVERSEE_LLM_DEBUG", "false")):
                masked = "" if not cfg["api_key"] else ("***" + cfg["api_key"][-4:])
                print(
                    f"ðŸ§­ Overseeé…ç½®: enabled={cfg['enabled']}, model={cfg['model']}, base_url={'set' if cfg['base_url'] else 'default'}, "
                    f"temperature={cfg['temperature']}, timeout={cfg['timeout']}s, api_key={masked}"
                )
        except Exception:
            pass
        return cfg
    except Exception:
        return {
            "enabled": False,
            "api_key": "",
            "base_url": "",
            "model": "",
            "temperature": 0.1,
            "timeout": 10,
        }


async def is_quant_by_oversee(raw_text: str) -> Optional[bool]:
    """ä½¿ç”¨åˆ¤åˆ«LLMåˆ¤æ–­æ˜¯å¦é‡åŒ–éœ€æ±‚ã€‚
    è¿”å›ž True/Falseï¼›è‹¥ä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œè¿”å›ž Noneï¼ˆæœªçŸ¥ï¼‰ã€‚
    """
    try:
        if not isinstance(raw_text, str) or not raw_text.strip():
            return None
        cfg = _get_oversee_config()
        if not cfg.get("enabled") or not cfg.get("api_key") or not cfg.get("model"):
            return None

        prev_key = os.getenv("OPENAI_API_KEY")
        prev_base = os.getenv("OPENAI_BASE_URL")
        try:
            os.environ["OPENAI_API_KEY"] = cfg["api_key"]
            if cfg.get("base_url"):
                os.environ["OPENAI_BASE_URL"] = cfg["base_url"]

            clf = ChatOpenAI(
                model=cfg["model"],
                temperature=cfg["temperature"],
                timeout=cfg["timeout"],
                max_retries=1,
            )
            system_prompt = os.getenv("OVERSEE_LLM_SYSTEM_PROMPT", "").strip() or (
                "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è·¯ç”±åˆ¤åˆ«å™¨ï¼Œåªèƒ½å›žç­”â€˜æ˜¯â€™æˆ–â€˜å¦â€™ã€‚\n"
                "åˆ¤å®šç›®æ ‡ï¼šç”¨æˆ·æ˜¯å¦åœ¨è¯·æ±‚â€˜é‡åŒ–äº¤æ˜“/å›žæµ‹/å› å­/ç­–ç•¥ä»£ç â€™ï¼ˆå°¤å…¶æ’ç”Ÿ PTrader/HS PTrader å¹³å°ï¼‰ã€‚\n"
                "æ»¡è¶³ä»»ä¸€æ¡ä»¶å³å›žç­”â€˜æ˜¯â€™ï¼š\n"
                "- æ˜Žç¡®æåŠé‡åŒ–/å›žæµ‹/å› å­/æ‹©æ—¶/è‡ªåŠ¨äº¤æ˜“/äº¤æ˜“æœºå™¨äºº/ä¹°å…¥/å–å‡º/æ­¢æŸ/æ­¢ç›ˆ/ä»“ä½/ä¿¡å·\n"
                "- æ˜Žç¡®æåŠæ’ç”Ÿ/Ptrader/HS PTrader æˆ–å…¶API/ç”Ÿå‘½å‘¨æœŸå‡½æ•°ï¼ˆinitialize/handle_data/run_daily/run_interval/on_order_response/on_trade_response/after_trading_end/set_universe/set_benchmark/set_commission/order/order_targetï¼‰\n"
                "- æ¨¡ç³ŠæåŠâ€˜èµšé’±çš„ä»£ç /ä¼šèµšé’±çš„ä»£ç â€™ä¸”è¯­å¢ƒæ˜¯è‚¡ç¥¨/äº¤æ˜“ç›¸å…³\n"
                "ä»¥ä¸‹æƒ…å†µå›žç­”â€˜å¦â€™ï¼šä»…æ˜¯ä¸€èˆ¬æ€§è´¢ç»/è¡Œä¸š/å…¬å¸åˆ†æžï¼Œæ²¡æœ‰è¦æ±‚ç”Ÿæˆé‡åŒ–ç­–ç•¥ä»£ç æˆ–å›žæµ‹è„šæœ¬ã€‚\n"
            )
            msgs = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=raw_text.strip()),
            ]
            resp = await clf.ainvoke(msgs)
            content = (getattr(resp, "content", None) or "").strip()
            normalized = content.lower()
            decision: Optional[bool] = None
            if normalized:
                if normalized.startswith("æ˜¯") or normalized in {"yes", "y", "true", "æ˜¯"}:
                    decision = True
                elif normalized.startswith("å¦") or normalized in {"no", "n", "false", "å¦"}:
                    decision = False
            # è°ƒè¯•æ—¥å¿—
            try:
                if _is_truthy(os.getenv("OVERSEE_LLM_DEBUG", "false")):
                    print(f"ðŸ§ª Overseeåˆ¤åˆ«LLM: raw='{content}' => decision={decision}")
            except Exception:
                pass
            return decision
        finally:
            if prev_key is not None:
                os.environ["OPENAI_API_KEY"] = prev_key
            if prev_base is not None:
                os.environ["OPENAI_BASE_URL"] = prev_base
    except Exception:
        return None


