"""
MANHHH é¡¹ç›®æ•°æ®åˆ†æè„šæœ¬
åˆ†æç”¨æˆ·æ•°ã€ä¼šè¯æ•°ã€æé—®è¯äº‘ç­‰
"""

import os
import sys
import sqlite3
import json
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter
import matplotlib.pyplot as plt
import matplotlib
import jieba
from wordcloud import WordCloud
import pandas as pd
import numpy as np

# è®¾ç½®ä¸­æ–‡å­—ä½“ - ç¨ååœ¨ä¸‹è½½å­—ä½“åå†é…ç½®
# matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
# matplotlib.rcParams['axes.unicode_minus'] = False

# é¡¹ç›®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
DB_PATH = BASE_DIR / "chat_history.db"
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_DIR.mkdir(exist_ok=True)

# ä¸‹è½½å¹¶è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆç”¨äºè¯äº‘ï¼‰
FONT_PATH = Path(__file__).parent / "SimHei.ttf"

def download_chinese_font():
    """ä¸‹è½½ä¸­æ–‡å­—ä½“ç”¨äºè¯äº‘å’Œå›¾è¡¨"""
    if FONT_PATH.exists():
        print(f"âœ“ ä¸­æ–‡å­—ä½“å·²å­˜åœ¨: {FONT_PATH}")
        # é…ç½® matplotlib ä½¿ç”¨ä¸‹è½½çš„å­—ä½“
        from matplotlib import font_manager
        font_manager.fontManager.addfont(str(FONT_PATH))
        matplotlib.rcParams['font.sans-serif'] = ['Source Han Sans SC', 'SimHei', 'DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False
        return str(FONT_PATH)
    
    print("â¬‡ï¸ ä¸‹è½½ä¸­æ–‡å­—ä½“...")
    import urllib.request
    
    # ä½¿ç”¨å¼€æºå­—ä½“ï¼šæ€æºé»‘ä½“
    font_url = "https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Regular.otf"
    
    try:
        urllib.request.urlretrieve(font_url, FONT_PATH)
        print(f"âœ“ å­—ä½“ä¸‹è½½æˆåŠŸ: {FONT_PATH}")
        # é…ç½® matplotlib ä½¿ç”¨ä¸‹è½½çš„å­—ä½“
        from matplotlib import font_manager
        font_manager.fontManager.addfont(str(FONT_PATH))
        matplotlib.rcParams['font.sans-serif'] = ['Source Han Sans SC', 'SimHei', 'DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False
        return str(FONT_PATH)
    except Exception as e:
        print(f"âš ï¸ å­—ä½“ä¸‹è½½å¤±è´¥: {e}")
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
        system_fonts = [
            "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",
            "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf",
            "/System/Library/Fonts/PingFang.ttc",
            "C:\\Windows\\Fonts\\simhei.ttf"
        ]
        for font in system_fonts:
            if os.path.exists(font):
                print(f"âœ“ ä½¿ç”¨ç³»ç»Ÿå­—ä½“: {font}")
                # é…ç½® matplotlib ä½¿ç”¨ç³»ç»Ÿå­—ä½“
                from matplotlib import font_manager
                font_manager.fontManager.addfont(font)
                matplotlib.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'SimHei', 'DejaVu Sans']
                matplotlib.rcParams['axes.unicode_minus'] = False
                return font
        
        print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œè¯äº‘å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡")
        matplotlib.rcParams['axes.unicode_minus'] = False
        return None


def connect_db():
    """è¿æ¥æ•°æ®åº“"""
    if not DB_PATH.exists():
        print(f"âŒ æ•°æ®åº“ä¸å­˜åœ¨: {DB_PATH}")
        sys.exit(1)
    
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    return conn


def get_user_statistics(conn):
    """è·å–ç”¨æˆ·ç»Ÿè®¡"""
    cursor = conn.cursor()
    
    stats = {}
    
    # æ€»ç”¨æˆ·æ•°
    cursor.execute("SELECT COUNT(*) as count FROM users")
    stats['total_users'] = cursor.fetchone()['count']
    
    # æœ‰é‚®ç®±çš„ç”¨æˆ·æ•°
    cursor.execute("SELECT COUNT(*) as count FROM users WHERE email IS NOT NULL AND email != ''")
    stats['users_with_email'] = cursor.fetchone()['count']
    
    # é…ç½®äº† Tushare Token çš„ç”¨æˆ·æ•°
    cursor.execute("SELECT COUNT(*) as count FROM users WHERE tushare_token IS NOT NULL AND tushare_token != ''")
    stats['users_with_tushare'] = cursor.fetchone()['count']
    
    # å¯ç”¨äº† Tushare Token çš„ç”¨æˆ·æ•°
    cursor.execute("SELECT COUNT(*) as count FROM users WHERE tushare_token_enabled = 1")
    stats['users_tushare_enabled'] = cursor.fetchone()['count']
    
    # ç§¯åˆ†ç»Ÿè®¡
    cursor.execute("SELECT AVG(credits) as avg, MIN(credits) as min, MAX(credits) as max FROM users")
    row = cursor.fetchone()
    stats['credits_avg'] = round(row['avg'] or 0, 2)
    stats['credits_min'] = row['min'] or 0
    stats['credits_max'] = row['max'] or 0
    
    # æ³¨å†Œæ—¶é—´åˆ†å¸ƒï¼ˆæœ€è¿‘7å¤©ã€30å¤©ï¼‰
    cursor.execute("""
        SELECT 
            COUNT(*) as count,
            datetime(created_at, 'localtime') as create_time
        FROM users 
        WHERE created_at >= datetime('now', '-7 days')
    """)
    stats['new_users_7days'] = len(cursor.fetchall())
    
    cursor.execute("""
        SELECT COUNT(*) as count
        FROM users 
        WHERE created_at >= datetime('now', '-30 days')
    """)
    stats['new_users_30days'] = cursor.fetchone()['count']
    
    return stats


def get_chat_statistics(conn):
    """è·å–èŠå¤©ç»Ÿè®¡"""
    cursor = conn.cursor()
    
    stats = {}
    
    # æ€»ä¼šè¯æ•°
    cursor.execute("SELECT COUNT(DISTINCT session_id) as count FROM chat_records")
    stats['total_sessions'] = cursor.fetchone()['count']
    
    # æ€»å¯¹è¯æ•°
    cursor.execute("SELECT COUNT(DISTINCT session_id || '-' || conversation_id) as count FROM chat_records")
    stats['total_conversations'] = cursor.fetchone()['count']
    
    # æ€»æ¶ˆæ¯æ•°
    cursor.execute("SELECT COUNT(*) as count FROM chat_records")
    stats['total_messages'] = cursor.fetchone()['count']
    
    # æœ€è¿‘7å¤©ã€30å¤©çš„æ¶ˆæ¯æ•°
    cursor.execute("""
        SELECT COUNT(*) as count
        FROM chat_records 
        WHERE created_at >= datetime('now', '-7 days')
    """)
    stats['messages_7days'] = cursor.fetchone()['count']
    
    cursor.execute("""
        SELECT COUNT(*) as count
        FROM chat_records 
        WHERE created_at >= datetime('now', '-30 days')
    """)
    stats['messages_30days'] = cursor.fetchone()['count']
    
    # å¹³å‡æ¯ä¸ªç”¨æˆ·çš„æ¶ˆæ¯æ•°
    cursor.execute("""
        SELECT COUNT(*) * 1.0 / COUNT(DISTINCT username) as avg
        FROM chat_records
        WHERE username IS NOT NULL
    """)
    stats['avg_messages_per_user'] = round(cursor.fetchone()['avg'] or 0, 2)
    
    # æœ€æ´»è·ƒçš„ç”¨æˆ· TOP 10
    cursor.execute("""
        SELECT username, COUNT(*) as message_count
        FROM chat_records
        WHERE username IS NOT NULL
        GROUP BY username
        ORDER BY message_count DESC
        LIMIT 10
    """)
    stats['top_users'] = [dict(row) for row in cursor.fetchall()]
    
    return stats


def get_tool_statistics(conn):
    """è·å–å·¥å…·è°ƒç”¨ç»Ÿè®¡"""
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT mcp_tools_called
        FROM chat_records
        WHERE mcp_tools_called IS NOT NULL AND mcp_tools_called != '[]'
    """)
    
    tool_counter = Counter()
    
    for row in cursor.fetchall():
        try:
            tools = json.loads(row['mcp_tools_called'])
            for tool in tools:
                if isinstance(tool, dict):
                    tool_name = tool.get('name', 'unknown')
                    tool_counter[tool_name] += 1
        except:
            pass
    
    return dict(tool_counter.most_common(20))


def get_user_questions(conn):
    """è·å–æ‰€æœ‰ç”¨æˆ·æé—®"""
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT user_input
        FROM chat_records
        WHERE user_input IS NOT NULL AND user_input != ''
        ORDER BY created_at DESC
    """)
    
    questions = [row['user_input'] for row in cursor.fetchall()]
    return questions


def generate_wordcloud(text, output_path, title="è¯äº‘"):
    """ç”Ÿæˆè¯äº‘"""
    if not text or not text.strip():
        print(f"âš ï¸ æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡è¯äº‘ç”Ÿæˆ: {title}")
        return
    
    # ä¸‹è½½ä¸­æ–‡å­—ä½“
    font_path = download_chinese_font()
    
    # ä½¿ç”¨ç»“å·´åˆ†è¯
    words = jieba.cut(text)
    filtered_words = []
    
    # åœç”¨è¯
    stop_words = set([
        'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
        'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'å—',
        'ä¼š', 'èƒ½', 'æ²¡', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'ä¸º',
        'ç€', 'ä¸‹', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æ€ä¹ˆ', 'å¯ä»¥',
        'å§', 'å•Š', 'å‘¢', 'å“¦', 'å—¯', 'å“ˆ', 'å˜¿', 'å‘€', 'å§', 'ä¹ˆ', 'å—'
    ])
    
    for word in words:
        if len(word) > 1 and word not in stop_words and word.strip():
            filtered_words.append(word)
    
    text_for_cloud = ' '.join(filtered_words)
    
    if not text_for_cloud.strip():
        print(f"âš ï¸ åˆ†è¯åæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡è¯äº‘ç”Ÿæˆ: {title}")
        return
    
    # ç”Ÿæˆè¯äº‘
    try:
        wordcloud = WordCloud(
            width=1600,
            height=800,
            background_color='white',
            font_path=font_path,
            max_words=200,
            relative_scaling=0.5,
            colormap='viridis'
        ).generate(text_for_cloud)
        
        plt.figure(figsize=(16, 8))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(title, fontsize=20, pad=20)
        plt.axis('off')
        plt.tight_layout(pad=0)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ è¯äº‘å·²ç”Ÿæˆ: {output_path}")
    except Exception as e:
        print(f"âŒ è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")


def plot_user_growth(conn):
    """ç»˜åˆ¶ç”¨æˆ·å¢é•¿è¶‹åŠ¿"""
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT DATE(created_at) as date, COUNT(*) as count
        FROM users
        GROUP BY DATE(created_at)
        ORDER BY date
    """)
    
    data = cursor.fetchall()
    
    if not data:
        print("âš ï¸ æ²¡æœ‰ç”¨æˆ·æ•°æ®")
        return
    
    dates = [row['date'] for row in data]
    counts = [row['count'] for row in data]
    cumulative = np.cumsum(counts)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # æ¯æ—¥æ–°å¢
    ax1.bar(dates, counts, color='skyblue', alpha=0.7)
    ax1.set_title('æ¯æ—¥æ–°å¢ç”¨æˆ·', fontsize=14, pad=10)
    ax1.set_xlabel('æ—¥æœŸ')
    ax1.set_ylabel('æ–°å¢ç”¨æˆ·æ•°')
    ax1.grid(True, alpha=0.3)
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    # ç´¯è®¡ç”¨æˆ·
    ax2.plot(dates, cumulative, marker='o', color='coral', linewidth=2)
    ax2.fill_between(range(len(dates)), cumulative, alpha=0.3, color='coral')
    ax2.set_title('ç´¯è®¡ç”¨æˆ·å¢é•¿', fontsize=14, pad=10)
    ax2.set_xlabel('æ—¥æœŸ')
    ax2.set_ylabel('ç´¯è®¡ç”¨æˆ·æ•°')
    ax2.grid(True, alpha=0.3)
    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / 'user_growth.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ“ ç”¨æˆ·å¢é•¿å›¾å·²ç”Ÿæˆ: {output_path}")


def plot_chat_activity(conn):
    """ç»˜åˆ¶èŠå¤©æ´»è·ƒåº¦"""
    cursor = conn.cursor()
    
    # æŒ‰æ—¥æœŸç»Ÿè®¡æ¶ˆæ¯æ•°
    cursor.execute("""
        SELECT DATE(created_at) as date, COUNT(*) as count
        FROM chat_records
        GROUP BY DATE(created_at)
        ORDER BY date DESC
        LIMIT 30
    """)
    
    data = cursor.fetchall()
    
    if not data:
        print("âš ï¸ æ²¡æœ‰èŠå¤©æ•°æ®")
        return
    
    data = list(reversed(data))  # æŒ‰æ—¶é—´æ­£åº
    dates = [row['date'] for row in data]
    counts = [row['count'] for row in data]
    
    plt.figure(figsize=(14, 6))
    plt.bar(dates, counts, color='mediumseagreen', alpha=0.7)
    plt.title('æœ€è¿‘30å¤©èŠå¤©æ´»è·ƒåº¦', fontsize=14, pad=10)
    plt.xlabel('æ—¥æœŸ')
    plt.ylabel('æ¶ˆæ¯æ•°')
    plt.xticks(rotation=45, ha='right')
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    
    output_path = OUTPUT_DIR / 'chat_activity.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ“ èŠå¤©æ´»è·ƒåº¦å›¾å·²ç”Ÿæˆ: {output_path}")


def plot_tool_usage(tool_stats):
    """ç»˜åˆ¶å·¥å…·ä½¿ç”¨ç»Ÿè®¡"""
    if not tool_stats:
        print("âš ï¸ æ²¡æœ‰å·¥å…·è°ƒç”¨æ•°æ®")
        return
    
    tools = list(tool_stats.keys())[:15]  # å–å‰15ä¸ª
    counts = [tool_stats[t] for t in tools]
    
    plt.figure(figsize=(12, 8))
    bars = plt.barh(tools, counts, color='steelblue', alpha=0.7)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, count) in enumerate(zip(bars, counts)):
        plt.text(count, i, f' {count}', va='center', fontsize=10)
    
    plt.title('å·¥å…·è°ƒç”¨æ¬¡æ•°ç»Ÿè®¡ (TOP 15)', fontsize=14, pad=10)
    plt.xlabel('è°ƒç”¨æ¬¡æ•°')
    plt.ylabel('å·¥å…·åç§°')
    plt.gca().invert_yaxis()
    plt.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    
    output_path = OUTPUT_DIR / 'tool_usage.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ“ å·¥å…·ä½¿ç”¨ç»Ÿè®¡å›¾å·²ç”Ÿæˆ: {output_path}")


def generate_report(user_stats, chat_stats, tool_stats):
    """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
    report = []
    report.append("=" * 60)
    report.append("MANHHH é¡¹ç›®æ•°æ®åˆ†ææŠ¥å‘Š")
    report.append("=" * 60)
    report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    report.append("ã€ç”¨æˆ·ç»Ÿè®¡ã€‘")
    report.append(f"  æ€»ç”¨æˆ·æ•°: {user_stats['total_users']}")
    report.append(f"  æœ‰é‚®ç®±ç”¨æˆ·: {user_stats['users_with_email']}")
    report.append(f"  é…ç½®äº† Tushare Token: {user_stats['users_with_tushare']}")
    report.append(f"  å¯ç”¨äº† Tushare Token: {user_stats['users_tushare_enabled']}")
    report.append(f"  æœ€è¿‘7å¤©æ–°å¢: {user_stats['new_users_7days']}")
    report.append(f"  æœ€è¿‘30å¤©æ–°å¢: {user_stats['new_users_30days']}")
    report.append(f"  ç§¯åˆ†å¹³å‡å€¼: {user_stats['credits_avg']}")
    report.append(f"  ç§¯åˆ†èŒƒå›´: {user_stats['credits_min']} - {user_stats['credits_max']}")
    report.append("")
    
    report.append("ã€èŠå¤©ç»Ÿè®¡ã€‘")
    report.append(f"  æ€»ä¼šè¯æ•°: {chat_stats['total_sessions']}")
    report.append(f"  æ€»å¯¹è¯æ•°: {chat_stats['total_conversations']}")
    report.append(f"  æ€»æ¶ˆæ¯æ•°: {chat_stats['total_messages']}")
    report.append(f"  æœ€è¿‘7å¤©æ¶ˆæ¯: {chat_stats['messages_7days']}")
    report.append(f"  æœ€è¿‘30å¤©æ¶ˆæ¯: {chat_stats['messages_30days']}")
    report.append(f"  äººå‡æ¶ˆæ¯æ•°: {chat_stats['avg_messages_per_user']}")
    report.append("")
    
    report.append("ã€æœ€æ´»è·ƒç”¨æˆ· TOP 10ã€‘")
    for i, user in enumerate(chat_stats['top_users'], 1):
        report.append(f"  {i}. {user['username']}: {user['message_count']} æ¡æ¶ˆæ¯")
    report.append("")
    
    report.append("ã€å·¥å…·è°ƒç”¨ç»Ÿè®¡ TOP 10ã€‘")
    for i, (tool, count) in enumerate(list(tool_stats.items())[:10], 1):
        report.append(f"  {i}. {tool}: {count} æ¬¡")
    report.append("")
    
    report.append("=" * 60)
    report.append("åˆ†æå®Œæˆï¼")
    report.append("=" * 60)
    
    report_text = '\n'.join(report)
    
    # ä¿å­˜æŠ¥å‘Š
    output_path = OUTPUT_DIR / 'analysis_report.txt'
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(report_text)
    print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    return report_text


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ•°æ®åˆ†æ...")
    print(f"ğŸ“Š æ•°æ®åº“è·¯å¾„: {DB_PATH}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("")
    
    # é¦–å…ˆä¸‹è½½å¹¶é…ç½®ä¸­æ–‡å­—ä½“
    print("ğŸ”¤ é…ç½®ä¸­æ–‡å­—ä½“...")
    download_chinese_font()
    
    # è¿æ¥æ•°æ®åº“
    conn = connect_db()
    
    try:
        # 1. ç”¨æˆ·ç»Ÿè®¡
        print("ğŸ“ˆ åˆ†æç”¨æˆ·ç»Ÿè®¡...")
        user_stats = get_user_statistics(conn)
        
        # 2. èŠå¤©ç»Ÿè®¡
        print("ğŸ’¬ åˆ†æèŠå¤©ç»Ÿè®¡...")
        chat_stats = get_chat_statistics(conn)
        
        # 3. å·¥å…·ç»Ÿè®¡
        print("ğŸ”§ åˆ†æå·¥å…·ä½¿ç”¨...")
        tool_stats = get_tool_statistics(conn)
        
        # 4. è·å–ç”¨æˆ·é—®é¢˜
        print("â“ æå–ç”¨æˆ·æé—®...")
        questions = get_user_questions(conn)
        all_questions_text = '\n'.join(questions)
        
        # 5. ç”Ÿæˆè¯äº‘ï¼ˆç¡®ä¿å­—ä½“å·²é…ç½®ï¼‰
        print("â˜ï¸ ç”Ÿæˆè¯äº‘...")
        generate_wordcloud(
            all_questions_text,
            OUTPUT_DIR / 'questions_wordcloud.png',
            'ç”¨æˆ·æé—®è¯äº‘'
        )
        
        # 6. ç»˜åˆ¶å›¾è¡¨
        print("ğŸ“Š ç»˜åˆ¶ç»Ÿè®¡å›¾è¡¨...")
        plot_user_growth(conn)
        plot_chat_activity(conn)
        plot_tool_usage(tool_stats)
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        generate_report(user_stats, chat_stats, tool_stats)
        
        # 8. ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON
        print("ğŸ’¾ ä¿å­˜åŸå§‹æ•°æ®...")
        data = {
            'user_stats': user_stats,
            'chat_stats': chat_stats,
            'tool_stats': tool_stats,
            'generated_at': datetime.now().isoformat()
        }
        
        json_path = OUTPUT_DIR / 'analysis_data.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ åŸå§‹æ•°æ®å·²ä¿å­˜: {json_path}")
        
        print("\n" + "=" * 60)
        print("âœ… æ•°æ®åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
        print("=" * 60)
        
    finally:
        conn.close()


if __name__ == '__main__':
    main()

