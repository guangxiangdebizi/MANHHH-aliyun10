"""
æ¨¡å‹æ¡£ä½ç®¡ç†æ¨¡å—
"""

import os
from typing import Dict, Any, Optional
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv, find_dotenv


class ModelManager:
    """æ¨¡å‹æ¡£ä½ç®¡ç†å™¨"""
    
    def __init__(self):
        # ç¡®ä¿.envæ–‡ä»¶è¢«æ­£ç¡®åŠ è½½
        try:
            env_path = find_dotenv()
            if env_path:
                load_dotenv(env_path, override=False)
                print(f"âœ… ModelManager æˆåŠŸåŠ è½½ .env æ–‡ä»¶: {env_path}")
            else:
                print("âš ï¸ ModelManager æœªæ‰¾åˆ° .env æ–‡ä»¶")
        except Exception as e:
            print(f"âš ï¸ ModelManager åŠ è½½ .env æ–‡ä»¶å¤±è´¥: {e}")
        
        self.llm_profiles = self._load_llm_profiles_from_env()
        self.default_profile_id = os.getenv("LLM_DEFAULT", "default").strip() or "default"
        if self.default_profile_id not in self.llm_profiles:
            self.default_profile_id = "default"
        self._llm_cache: Dict[str, Dict[str, Any]] = {}
        
        # æ•°å€¼é…ç½®ï¼Œå¸¦é»˜è®¤
        try:
            self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
        except Exception:
            self.temperature = 0.2
        try:
            self.timeout = int(os.getenv("OPENAI_TIMEOUT", "60"))
        except Exception:
            self.timeout = 60
        
        # åŸºç¡€æ¨¡å‹é…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY", "").strip()
        self.base_url = os.getenv("OPENAI_BASE_URL", "").strip()
        self.model_name = os.getenv("OPENAI_MODEL", os.getenv("OPENAI_MODEL_NAME", "deepseek-chat")).strip()

    def _load_llm_profiles_from_env(self) -> Dict[str, Dict[str, Any]]:
        """ä»ç¯å¢ƒå˜é‡è§£æå¤šæ¨¡å‹æ¡£ä½é…ç½®ã€‚
        çº¦å®šï¼š
        - LLM_PROFILES=profile1,profile2
        - æ¯ä¸ªæ¡£ä½å˜é‡ï¼š
          LLM_<ID>_LABELã€LLM_<ID>_API_KEYã€LLM_<ID>_BASE_URLã€LLM_<ID>_MODELã€
          ï¼ˆå¯é€‰ï¼‰LLM_<ID>_TEMPERATUREã€LLM_<ID>_TIMEOUTã€LLM_<ID>_SYSTEM_PROMPT
        - åŒæ—¶æä¾›ä¸€ä¸ªå‘åå…¼å®¹çš„ default æ¡£ä½ï¼Œæ¥è‡ª OPENAI_* å˜é‡
        """
        profiles: Dict[str, Dict[str, Any]] = {}

        # default æ¡£ä½ï¼ˆå‘åå…¼å®¹ç°æœ‰ OPENAI_*ï¼‰
        profiles["default"] = {
            "id": "default",
            "label": os.getenv("LLM_DEFAULT_LABEL", "Default"),
            "api_key": os.getenv("OPENAI_API_KEY", "").strip(),
            "base_url": os.getenv("OPENAI_BASE_URL", "").strip(),
            "model": os.getenv("OPENAI_MODEL", os.getenv("OPENAI_MODEL_NAME", "deepseek-chat")).strip(),
            "temperature": float(os.getenv("OPENAI_TEMPERATURE", "0.2")),
            "timeout": int(os.getenv("OPENAI_TIMEOUT", "60")),
            "system_prompt": os.getenv("LLM_DEFAULT_SYSTEM_PROMPT", "").strip(),
            # é»˜è®¤æ¡£ä½ç±»å‹ä¸ºæ™®é€šæ¨¡å‹
            "kind": os.getenv("LLM_DEFAULT_KIND", "model").strip() or "model",
            # å…¼å®¹ agent æ‰©å±•å­—æ®µï¼ˆé»˜è®¤æ¡£ä½æ­£å¸¸ä¸ºç©ºï¼‰
            "agent_file": os.getenv("LLM_DEFAULT_AGENT_FILE", "").strip(),
            "backing_profile": os.getenv("LLM_DEFAULT_BACKING_PROFILE", "").strip(),
        }

        ids_raw = os.getenv("LLM_PROFILES", "").strip()
        if ids_raw:
            for pid in [x.strip() for x in ids_raw.split(",") if x.strip()]:
                pid_upper = pid.upper()
                kind = (os.getenv(f"LLM_{pid_upper}_KIND", "model").strip() or "model").lower()

                api_key = os.getenv(f"LLM_{pid_upper}_API_KEY", "").strip()
                model_name = os.getenv(f"LLM_{pid_upper}_MODEL", "").strip()
                base_url = os.getenv(f"LLM_{pid_upper}_BASE_URL", "").strip()
                label = os.getenv(f"LLM_{pid_upper}_LABEL", pid)
                try:
                    temperature = float(os.getenv(f"LLM_{pid_upper}_TEMPERATURE", os.getenv("OPENAI_TEMPERATURE", "0.2")))
                except Exception:
                    temperature = 0.2
                try:
                    timeout = int(os.getenv(f"LLM_{pid_upper}_TIMEOUT", os.getenv("OPENAI_TIMEOUT", "60")))
                except Exception:
                    timeout = 60
                system_prompt = os.getenv(f"LLM_{pid_upper}_SYSTEM_PROMPT", "").strip()
                agent_file = os.getenv(f"LLM_{pid_upper}_AGENT_FILE", "").strip()
                backing_profile = os.getenv(f"LLM_{pid_upper}_BACKING_PROFILE", "").strip()

                # é agent æ¡£ä½ï¼šæ²¡æœ‰ api_key æˆ– model åˆ™è·³è¿‡
                if kind != "agent" and (not api_key or not model_name):
                    continue

                profiles[pid] = {
                    "id": pid,
                    "label": label,
                    "api_key": api_key,
                    "base_url": base_url,
                    "model": model_name,
                    "temperature": temperature,
                    "timeout": timeout,
                    "system_prompt": system_prompt,
                    "kind": kind,
                    "agent_file": agent_file,
                    "backing_profile": backing_profile,
                }

        return profiles

    def get_models_info(self) -> Dict[str, Any]:
        """å¯¹å¤–æš´éœ²çš„æ¨¡å‹æ¡£ä½ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰ã€‚"""
        profiles = self.llm_profiles or {}
        ids = list(profiles.keys())
        non_default_ids = [pid for pid in ids if pid != "default"]

        # è®¡ç®—æœ‰æ•ˆé»˜è®¤æ¡£ä½ï¼šä¼˜å…ˆé‡‡ç”¨ LLM_DEFAULT æŒ‡å®šä¸”å­˜åœ¨çš„IDï¼›
        # å¦åˆ™è‹¥å­˜åœ¨é default æ¡£ä½ï¼Œå–ç¬¬ä¸€ä¸ªï¼›å¦åˆ™åªèƒ½æ˜¯ defaultï¼ˆå•æ¨¡å‹æ—§å…¼å®¹ï¼‰
        if self.default_profile_id and self.default_profile_id != "default" and self.default_profile_id in profiles:
            effective_default = self.default_profile_id
        elif non_default_ids:
            effective_default = non_default_ids[0]
        else:
            effective_default = "default"

        # å±•ç¤ºç­–ç•¥ï¼š
        # - è‹¥å­˜åœ¨ä»»æ„é default æ¡£ä½ï¼Œåˆ™å®Œå…¨éšè— defaultï¼ˆå®ƒåªä½œä¸ºåˆ«å/å›é€€ï¼Œä¸å•ç‹¬æ˜¾ç¤ºï¼‰ã€‚
        # - è‹¥åªæœ‰ default ä¸€ä¸ªæ¡£ä½ï¼Œåˆ™æ˜¾ç¤ºå®ƒï¼ˆæ—§ç‰ˆå•æ¨¡å‹åœºæ™¯ï¼‰ã€‚
        show_ids = non_default_ids if non_default_ids else (["default"] if "default" in profiles else [])

        # å±•ç¤ºæ‰€æœ‰åœ¨ LLM_PROFILES ä¸­å£°æ˜çš„æ¡£ä½ï¼ˆä¸å†æŒ‰ base_url+model å»é‡ï¼‰
        models = []
        for pid in show_ids:
            cfg = profiles.get(pid, {})
            kind = cfg.get("kind", "model")
            model_for_display = cfg.get("model", "")
            if kind == "agent":
                backing_id = cfg.get("backing_profile")
                if backing_id and backing_id in profiles:
                    model_for_display = profiles[backing_id].get("model", model_for_display)
            models.append({
                "id": pid,
                "label": cfg.get("label", pid),
                "model": model_for_display,
                "is_default": pid == effective_default,
                "type": kind,
                "is_agent": (kind == "agent"),
            })

        # æç«¯å…œåº•ï¼šå¦‚æœæœ€ç»ˆä¸€ä¸ªéƒ½æ²¡æœ‰ï¼ˆç†è®ºä¸ä¼šå‘ç”Ÿï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨ä¸é»˜è®¤ID
        return {"models": models, "default": effective_default}

    def get_current_model_key(self, session_contexts: Dict[str, Dict[str, Any]], session_id: Optional[str] = None) -> str:
        """è·å–å½“å‰ä¼šè¯ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†ï¼ˆç”¨äºè®°å½•å¤šæ¨¡æ€æ”¯æŒæƒ…å†µï¼‰"""
        try:
            profile_id = None
            if session_id and session_contexts.get(session_id):
                profile_id = session_contexts[session_id].get("model") or session_contexts[session_id].get("llm_profile")
            if not profile_id:
                profile_id = self.default_profile_id
            cfg = self.llm_profiles.get(profile_id, {})
            model_name = cfg.get("model", "")
            base_url = cfg.get("base_url", "")
            return f"{model_name}@{base_url}"
        except Exception:
            return "unknown"

    def get_system_prompt(self, session_contexts: Dict[str, Dict[str, Any]], session_id: Optional[str] = None) -> str:
        """è·å–å½“å‰ä¼šè¯ä½¿ç”¨çš„æ¨¡å‹çš„ç³»ç»Ÿæç¤ºè¯"""
        try:
            profile_id = None
            if session_id and session_contexts.get(session_id):
                profile_id = session_contexts[session_id].get("model") or session_contexts[session_id].get("llm_profile")
            if not profile_id:
                profile_id = self.default_profile_id
            
            print(f"ğŸ” è·å–ç³»ç»Ÿæç¤ºè¯: session_id={session_id}, profile_id={profile_id}")
            
            cfg = self.llm_profiles.get(profile_id, {})
            print(f"ğŸ” æ‰¾åˆ°é…ç½®: {bool(cfg)}")
            
            # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ç³»ç»Ÿæç¤ºè¯
            system_prompt = cfg.get("system_prompt", "")
            print(f"ğŸ” ç¯å¢ƒå˜é‡æç¤ºè¯é•¿åº¦: {len(system_prompt)}")
            
            # å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»å¯¹åº”çš„æ–‡ä»¶ä¸­è¯»å–
            if not system_prompt:
                system_prompt = self._load_prompt_from_file(profile_id)
                print(f"ğŸ” æ–‡ä»¶æç¤ºè¯é•¿åº¦: {len(system_prompt)}")
            
            # å¦‚æœå½“å‰æ¨¡å‹æ²¡æœ‰é…ç½®ç³»ç»Ÿæç¤ºè¯ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œç”±è°ƒç”¨æ–¹ä½¿ç”¨é»˜è®¤é€»è¾‘
            return system_prompt
        except Exception as e:
            print(f"âŒ è·å–ç³»ç»Ÿæç¤ºè¯å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _load_prompt_from_file(self, profile_id: str) -> str:
        """ä»æ–‡ä»¶ä¸­åŠ è½½ç³»ç»Ÿæç¤ºè¯"""
        try:
            import importlib.util
            import os
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            prompt_file_path = os.path.join(
                os.path.dirname(__file__), 
                "..", 
                "prompts", 
                f"LLM_{profile_id}_SYSTEM_PROMPT.py"
            )
            
            if not os.path.exists(prompt_file_path):
                return ""
            
            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            spec = importlib.util.spec_from_file_location(
                f"prompt_{profile_id}", prompt_file_path
            )
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            
            # è·å– SYSTEM_PROMPT å˜é‡
            return getattr(module, "SYSTEM_PROMPT", "")
        
        except Exception as e:
            print(f"âš ï¸ ä»æ–‡ä»¶åŠ è½½æç¤ºè¯å¤±è´¥ ({profile_id}): {e}")
            return ""

    def get_or_create_llm_instances(self, profile_id: str, tools: list) -> Dict[str, Any]:
        """æ ¹æ®æ¡£ä½è·å–/åˆ›å»ºå¯¹åº”çš„ LLM å®ä¾‹é›†åˆï¼šllmã€llm_nontoolã€llm_toolsã€‚"""
        pid = profile_id or self.default_profile_id
        if pid not in self.llm_profiles:
            pid = self.default_profile_id

        if pid in self._llm_cache:
            return self._llm_cache[pid]

        cfg = self.llm_profiles[pid]

        # ä¸´æ—¶åˆ‡æ¢ç¯å¢ƒå˜é‡ï¼Œæ„é€ å®ä¾‹
        prev_key = os.getenv("OPENAI_API_KEY")
        prev_base = os.getenv("OPENAI_BASE_URL")
        try:
            if cfg.get("api_key"):
                os.environ["OPENAI_API_KEY"] = cfg["api_key"]
            if cfg.get("base_url"):
                os.environ["OPENAI_BASE_URL"] = cfg["base_url"]

            base_llm = ChatOpenAI(
                model=cfg.get("model", self.model_name),
                temperature=cfg.get("temperature", self.temperature),
                timeout=cfg.get("timeout", self.timeout),
                max_retries=3,
            )
            llm_nontool = ChatOpenAI(
                model=cfg.get("model", self.model_name),
                temperature=cfg.get("temperature", self.temperature),
                timeout=cfg.get("timeout", self.timeout),
                max_retries=3,
            )
            llm_tools = base_llm.bind_tools(tools)
        finally:
            # è¿˜åŸç¯å¢ƒï¼Œé¿å…å½±å“å…¶ä»–é€»è¾‘
            if prev_key is not None:
                os.environ["OPENAI_API_KEY"] = prev_key
            if prev_base is not None:
                os.environ["OPENAI_BASE_URL"] = prev_base

        bundle = {"llm": base_llm, "llm_nontool": llm_nontool, "llm_tools": llm_tools}
        self._llm_cache[pid] = bundle
        return bundle
