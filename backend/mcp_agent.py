"""
MCPæ™ºèƒ½ä½“å°è£… - ä¸ºWebåç«¯ä½¿ç”¨
åŸºäº test.py ä¸­çš„ SimpleMCPAgentï¼Œä¼˜åŒ–ä¸ºé€‚åˆWebSocketæµå¼æ¨é€çš„ç‰ˆæœ¬
"""

import os
import json
import asyncio
from typing import Dict, List, Any, AsyncGenerator, Optional
from pathlib import Path
from datetime import datetime, timedelta

from dotenv import load_dotenv, find_dotenv
import re
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
import contextvars
import pymysql

# å¯¼å…¥æ¨¡å—åŒ–ç»„ä»¶
from mcp_modules.config import MCPConfig
from mcp_modules.multimodal import MultimodalProcessor
from mcp_modules.model_manager import ModelManager
from mcp_modules.message_processor import MessageProcessor
from get_mcp_tools import MCPToolsManager
from mcp_modules.agent_orchestrator import AgentOrchestrator

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. MCPé…ç½®ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å·²ç§»è‡³ mcp_agent/config.py


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Webç‰ˆMCPæ™ºèƒ½ä½“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class WebMCPAgent:
    """Webç‰ˆMCPæ™ºèƒ½ä½“ - æ”¯æŒæµå¼æ¨é€"""

    def __init__(self):
        # ä¿®å¤ï¼šä½¿ç”¨backendç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶
        config_path = Path(__file__).parent / "mcp.json"
        self.config = MCPConfig(str(config_path))
        self.llm = None
        self.llm_tools = None  # ç»‘å®šå·¥å…·ç”¨äºåˆ¤å®šä¸å·¥å…·é˜¶æ®µ
        self.llm_nontool = None  # æ— å·¥å…·å®ä¾‹ï¼Œä»…ç”¨äºå·¥å…·å†…éƒ¨å¦‚SQLé‡å†™
        
        # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
        self.tools_manager = MCPToolsManager()
        # å…¼å®¹æ€§å±æ€§ï¼ˆæŒ‡å‘å·¥å…·ç®¡ç†å™¨çš„å±æ€§ï¼‰
        self.tools = self.tools_manager.tools
        self.tools_by_server = self.tools_manager.tools_by_server
        self.server_configs = self.tools_manager.server_configs
        self.mcp_client = self.tools_manager.mcp_client

        # åŠ è½½ .env å¹¶è®¾ç½®APIç¯å¢ƒå˜é‡ï¼ˆè¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼‰
        try:
            load_dotenv(find_dotenv(), override=True)
        except Exception:
            # å¿½ç•¥ .env åŠ è½½é”™è¯¯ï¼Œç»§ç»­ä»ç³»ç»Ÿç¯å¢ƒè¯»å–
            pass

        # åˆå§‹åŒ–æ¨¡å—åŒ–ç»„ä»¶
        self.model_manager = ModelManager()

        # ä»æ¨¡å‹ç®¡ç†å™¨è·å–é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        self.api_key = self.model_manager.api_key
        self.base_url = self.model_manager.base_url
        self.model_name = self.model_manager.model_name
        self.llm_profiles = self.model_manager.llm_profiles
        self.default_profile_id = self.model_manager.default_profile_id
        self.temperature = self.model_manager.temperature
        self.timeout = self.model_manager.timeout

        # å°†å…³é”®é…ç½®åŒæ­¥åˆ°ç¯å¢ƒï¼ˆä¾›åº•å±‚SDKä½¿ç”¨ï¼‰ï¼Œä¸è¦†ç›–å¤–éƒ¨å·²è®¾å€¼
        if self.api_key and not os.getenv("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = self.api_key
        if self.base_url and not os.getenv("OPENAI_BASE_URL"):
            os.environ["OPENAI_BASE_URL"] = self.base_url

        # ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆå­˜æ”¾æ¯ä¸ª session çš„ msid ç­‰ï¼‰
        self.session_contexts: Dict[str, Dict[str, Any]] = {}
        
        # å†å²å›¾ç‰‡é…ç½®
        try:
            public_base_url = os.getenv("PUBLIC_BASE_URL", "").strip()
            history_image_max_file_bytes = int(os.getenv("HISTORY_IMAGE_MAX_FILE_BYTES", str(2 * 1024 * 1024)))
        except Exception:
            public_base_url = ""
            history_image_max_file_bytes = 2 * 1024 * 1024
            
        self.multimodal_processor = MultimodalProcessor(public_base_url, history_image_max_file_bytes)
        
        try:
            history_images_max_total = int(os.getenv("HISTORY_IMAGES_MAX_TOTAL", "6"))
            history_images_max_per_record = int(os.getenv("HISTORY_IMAGES_MAX_PER_RECORD", "3"))
        except Exception:
            history_images_max_total = 6
            history_images_max_per_record = 3
            
        self.message_processor = MessageProcessor(self.multimodal_processor, history_images_max_total, history_images_max_per_record)
        # å½“å‰ä¼šè¯IDä¸Šä¸‹æ–‡å˜é‡ï¼ˆç”¨äºå·¥å…·åœ¨è¿è¡Œæ—¶è¯†åˆ«ä¼šè¯ï¼‰
        self._current_session_id_ctx: contextvars.ContextVar = contextvars.ContextVar("current_session_id", default=None)
        
        # è®°å½•ä¸æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼ˆé¿å…é‡å¤å°è¯•ï¼‰
        self._non_multimodal_models: set = set()

        # Agent ç¼–æ’å™¨
        self.agent_orchestrator = AgentOrchestrator(
            tools_manager=self.tools_manager,
            message_processor=self.message_processor,
            model_manager=self.model_manager,
            get_llm_bundle_fn=self._get_or_create_llm_instances,
            current_session_id_ctx=self._current_session_id_ctx,
        )

        # æ•°æ®åº“é…ç½®ï¼ˆä»ç¯å¢ƒè¯»å–ï¼Œæä¾›é»˜è®¤å€¼ï¼‰
        self.db_host = os.getenv("DB_HOST", "18.119.46.208")
        self.db_user = os.getenv("DB_USER", "root")
        self.db_password = os.getenv("DB_PASSWORD", "zkshi0101")
        self.db_name = os.getenv("DB_NAME", "ry_vuebak")
        try:
            self.db_port = int(os.getenv("DB_PORT", "3306"))
        except Exception:
            self.db_port = 3306

        # ä¸Šä¸‹æ–‡å˜é‡ä¸éå¤šæ¨¡æ€é›†åˆå·²åœ¨ä¸Šæ–¹åˆå§‹åŒ–

    # å·²ç§»è‡³ mcp_agent/model_manager.py



    # å·²ç§»è‡³ mcp_agent/multimodal.py

    def _get_current_model_key(self, session_id: Optional[str] = None) -> str:
        """è·å–å½“å‰ä¼šè¯ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†ï¼ˆç”¨äºè®°å½•å¤šæ¨¡æ€æ”¯æŒæƒ…å†µï¼‰"""
        return self.model_manager.get_current_model_key(self.session_contexts, session_id)

    def _convert_multimodal_to_text(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†å¤šæ¨¡æ€æ¶ˆæ¯è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ¶ˆæ¯"""
        return self.multimodal_processor.convert_multimodal_to_text(messages)

    def _is_multimodal_error(self, error_str: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤šæ¨¡æ€æ ¼å¼ä¸æ”¯æŒçš„é”™è¯¯"""
        return MultimodalProcessor.is_multimodal_error(error_str)


    def get_models_info(self) -> Dict[str, Any]:
        """å¯¹å¤–æš´éœ²çš„æ¨¡å‹æ¡£ä½ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰ã€‚"""
        profiles = self.llm_profiles or {}
        ids = list(profiles.keys())
        non_default_ids = [pid for pid in ids if pid != "default"]

        # è®¡ç®—æœ‰æ•ˆé»˜è®¤æ¡£ä½ï¼šä¼˜å…ˆé‡‡ç”¨ LLM_DEFAULT æŒ‡å®šä¸”å­˜åœ¨çš„IDï¼›
        # å¦åˆ™è‹¥å­˜åœ¨é default æ¡£ä½ï¼Œå–ç¬¬ä¸€ä¸ªï¼›å¦åˆ™åªèƒ½æ˜¯ defaultï¼ˆå•æ¨¡å‹æ—§å…¼å®¹ï¼‰
        if self.default_profile_id and self.default_profile_id != "default" and self.default_profile_id in profiles:
            effective_default = self.default_profile_id
        elif non_default_ids:
            effective_default = non_default_ids[0]
        else:
            effective_default = "default"

        # å±•ç¤ºç­–ç•¥ï¼š
        # - è‹¥å­˜åœ¨ä»»æ„é default æ¡£ä½ï¼Œåˆ™å®Œå…¨éšè— defaultï¼ˆå®ƒåªä½œä¸ºåˆ«å/å›é€€ï¼Œä¸å•ç‹¬æ˜¾ç¤ºï¼‰ã€‚
        # - è‹¥åªæœ‰ default ä¸€ä¸ªæ¡£ä½ï¼Œåˆ™æ˜¾ç¤ºå®ƒï¼ˆæ—§ç‰ˆå•æ¨¡å‹åœºæ™¯ï¼‰ã€‚
        show_ids = non_default_ids if non_default_ids else (["default"] if "default" in profiles else [])

        # å±•ç¤ºæ‰€æœ‰åœ¨ LLM_PROFILES ä¸­å£°æ˜çš„æ¡£ä½ï¼ˆä¸å†æŒ‰ base_url+model å»é‡ï¼‰
        models = []
        for pid in show_ids:
            cfg = profiles.get(pid, {})
            models.append({
                "id": pid,
                "label": cfg.get("label", pid),
                "model": cfg.get("model", ""),
                "is_default": pid == effective_default,
            })

        # æç«¯å…œåº•ï¼šå¦‚æœæœ€ç»ˆä¸€ä¸ªéƒ½æ²¡æœ‰ï¼ˆç†è®ºä¸ä¼šå‘ç”Ÿï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨ä¸é»˜è®¤ID
        return {"models": models, "default": effective_default}

    def _get_or_create_llm_instances(self, profile_id: str) -> Dict[str, Any]:
        """æ ¹æ®æ¡£ä½è·å–/åˆ›å»ºå¯¹åº”çš„ LLM å®ä¾‹é›†åˆï¼šllmã€llm_nontoolã€llm_toolsã€‚"""
        return self.model_manager.get_or_create_llm_instances(profile_id, self.tools)

    async def initialize(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        try:
            # é€‰æ‹©å¯åŠ¨æ¡£ä½ï¼šä¼˜å…ˆ LLM_DEFAULT æŒ‡å®šçš„æ¡£ä½ï¼›å¦åˆ™é€‰æ‹©ä»»ä¸€å« api_key çš„æ¡£ä½ï¼›
            # è‹¥å‡æ— åˆ™å›é€€åˆ°ç¯å¢ƒå˜é‡ OPENAI_API_KEYï¼›ä»æ— åˆ™æŠ¥é”™
            startup_cfg = None
            # ä¼˜å…ˆé»˜è®¤æ¡£ä½
            if self.default_profile_id in self.llm_profiles:
                cfg = self.llm_profiles[self.default_profile_id]
                if cfg.get("api_key") and cfg.get("model"):
                    startup_cfg = cfg
            # å…¶æ¬¡ä»»æ„æœ‰æ•ˆæ¡£ä½
            if startup_cfg is None:
                for _pid, cfg in self.llm_profiles.items():
                    if _pid == "default":
                        continue
                    if cfg.get("api_key") and cfg.get("model"):
                        startup_cfg = cfg
                        break
            # æœ€åå›é€€åˆ°ç¯å¢ƒå˜é‡
            if startup_cfg is None and os.getenv("OPENAI_API_KEY"):
                startup_cfg = {
                    "api_key": os.getenv("OPENAI_API_KEY").strip(),
                    "base_url": os.getenv("OPENAI_BASE_URL", "").strip(),
                    "model": self.model_name,
                    "temperature": self.temperature,
                    "timeout": self.timeout,
                }
            if startup_cfg is None:
                raise RuntimeError("ç¼ºå°‘å¯ç”¨çš„æ¨¡å‹æ¡£ä½æˆ– OPENAI_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½® LLM_PROFILES å¯¹åº”çš„ *_API_KEY æˆ–æä¾› OPENAI_API_KEY")

            # ä¸´æ—¶å†™å…¥ç¯å¢ƒä¾›åº•å±‚ SDK ä½¿ç”¨
            if startup_cfg.get("api_key"):
                os.environ["OPENAI_API_KEY"] = startup_cfg["api_key"]
            if startup_cfg.get("base_url"):
                os.environ["OPENAI_BASE_URL"] = startup_cfg["base_url"]

            # ChatOpenAI æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– base_url
            base_llm = ChatOpenAI(
                model=startup_cfg.get("model", self.model_name),
                temperature=startup_cfg.get("temperature", self.temperature),
                timeout=startup_cfg.get("timeout", self.timeout),
                max_retries=3,
            )
            # ä¸»å¼•ç”¨å‘åå…¼å®¹
            self.llm = base_llm
            # æ— å·¥å…·å®ä¾‹ï¼šå½“å‰ä¸ base_llm ç›¸åŒï¼ˆæ— éœ€ç»‘å®šå·¥å…·ï¼‰ï¼Œä¾›å·¥å…·å†…éƒ¨è°ƒç”¨
            self.llm_nontool = ChatOpenAI(
                model=startup_cfg.get("model", self.model_name),
                temperature=startup_cfg.get("temperature", self.temperature),
                timeout=startup_cfg.get("timeout", self.timeout),
                max_retries=3,
            )

            # åŠ è½½MCPé…ç½®å¹¶è¿æ¥
            mcp_config = self.config.load_config()
            server_configs = mcp_config.get("servers", {})
            
            # å‡†å¤‡æ•°æ®åº“é…ç½®
            db_config = {
                'host': self.db_host,
                'user': self.db_user,
                'password': self.db_password,
                'name': self.db_name,
                'port': self.db_port
            }
            
            # ä½¿ç”¨å·¥å…·ç®¡ç†å™¨åˆå§‹åŒ–å·¥å…·
            tools_success = await self.tools_manager.initialize_mcp_tools(
                server_configs=server_configs,
                db_config=db_config,
                session_contexts=self.session_contexts,
                current_session_id_ctx=self._current_session_id_ctx,
                llm_nontool=self.llm_nontool
            )
            
            if not tools_success:
                print("âš ï¸ å·¥å…·åˆå§‹åŒ–å¤±è´¥ï¼Œä½†ç»§ç»­å¯åŠ¨")
            
            # æ›´æ–°å¼•ç”¨ï¼ˆå·¥å…·ç®¡ç†å™¨å¯èƒ½å·²æ›´æ–°è¿™äº›å±æ€§ï¼‰
            self.mcp_client = self.tools_manager.mcp_client

            # åˆ›å»ºå·¥å…·åˆ¤å®šå®ä¾‹ï¼ˆé»˜è®¤æ¡£ä½ï¼‰ï¼Œå…¶ä½™æ¡£ä½åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶æŒ‰éœ€åˆ›å»º
            self.llm_tools = base_llm.bind_tools(self.tools)

            print("ğŸ¤– Web MCPæ™ºèƒ½åŠ©æ‰‹å·²å¯åŠ¨ï¼")
            return True

        except Exception as e:
            import traceback
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            # å°è¯•æ¸…ç†å¯èƒ½çš„è¿æ¥
            if hasattr(self, 'mcp_client') and self.mcp_client:
                try:
                    await self.mcp_client.close()
                except:
                    pass
            return False

    async def reload_mcp_servers(self) -> bool:
        """é‡æ–°åŠ è½½ backend/mcp.json å¹¶é‡å»º MCP å®¢æˆ·ç«¯ä¸å·¥å…·é›†åˆã€‚
        ç”¨äºåœ¨è¿è¡Œæ—¶æ›´æ–°æœåŠ¡å™¨ headersï¼ˆä¾‹å¦‚ Tushare Tokenï¼‰ã€‚
        """
        try:
            # å…³é—­æ—§å®¢æˆ·ç«¯
            try:
                if self.tools_manager and self.tools_manager.mcp_client:
                    await self.tools_manager.close()
            except Exception:
                pass

            # é‡æ–°åŠ è½½é…ç½®
            mcp_config = self.config.load_config()
            server_configs = mcp_config.get("servers", {})

            # é‡æ–°åˆå§‹åŒ–å·¥å…·
            tools_success = await self.tools_manager.initialize_mcp_tools(
                server_configs=server_configs,
                db_config={
                    'host': self.db_host,
                    'user': self.db_user,
                    'password': self.db_password,
                    'name': self.db_name,
                    'port': self.db_port
                },
                session_contexts=self.session_contexts,
                current_session_id_ctx=self._current_session_id_ctx,
                llm_nontool=self.llm_nontool
            )
            if not tools_success:
                print("âš ï¸ å·¥å…·é‡è½½å¤±è´¥")
                return False

            # æ›´æ–°å¼•ç”¨
            self.mcp_client = self.tools_manager.mcp_client
            self.tools = self.tools_manager.tools
            self.tools_by_server = self.tools_manager.tools_by_server

            # é‡æ–°ç»‘å®šå·¥å…·åˆ°å½“å‰ LLM
            if self.llm:
                self.llm_tools = self.llm.bind_tools(self.tools)

            print("âœ… MCPæœåŠ¡å™¨é…ç½®å·²é‡è½½")
            return True
        except Exception as e:
            import traceback
            print(f"âŒ é‡è½½MCPæœåŠ¡å™¨å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def _get_tools_system_prompt(self) -> str:
        """ç”¨äºå·¥å…·åˆ¤å®š/æ‰§è¡Œé˜¶æ®µçš„ç³»ç»Ÿæç¤ºè¯ï¼šä»ç¯å¢ƒå˜é‡è¯»å–æˆ–ä½¿ç”¨é»˜è®¤æç¤ºè¯ã€‚"""
        # å°è¯•ä»æ¨¡å‹ç®¡ç†å™¨è·å–å½“å‰æ¨¡å‹çš„ç³»ç»Ÿæç¤ºè¯
        try:
            model_manager = ModelManager()
            # è·å–å½“å‰ä¼šè¯IDï¼Œä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡å˜é‡
            current_session_id = self._current_session_id_ctx.get(None)
            # ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹ä¼˜å…ˆä½¿ç”¨è‡ªèº«æç¤ºè¯
            try:
                pf = None
                if current_session_id and self.session_contexts.get(current_session_id):
                    pf = self.session_contexts[current_session_id].get("model")
                if pf and str(pf).startswith("user-"):
                    user_map = self.session_contexts.get(current_session_id, {}).get("user_models") or {}
                    cfg = user_map.get(pf)
                    if cfg and (cfg.get("system_prompt") or "").strip():
                        custom_prompt = (cfg.get("system_prompt") or "").strip()
                    else:
                        custom_prompt = model_manager.get_system_prompt(self.session_contexts, current_session_id)
                else:
                    custom_prompt = model_manager.get_system_prompt(self.session_contexts, current_session_id)
            except Exception:
                custom_prompt = model_manager.get_system_prompt(self.session_contexts, current_session_id)
            
            if custom_prompt:
                # å¦‚æœæœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œå®‰å…¨åœ°æ›¿æ¢å ä½ç¬¦ï¼ˆé¿å…ä¸ JSON/mermaid èŠ±æ‹¬å·å†²çªï¼‰
                now = datetime.now()
                current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
                current_time = now.strftime("%H:%M:%S")
                current_datetime = now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
                current_weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
                current_hour = now.hour
                current_minute = now.minute
                current_timestamp = int(now.timestamp())

                replacements = {
                    "current_date": current_date,
                    "current_time": current_time,
                    "current_datetime": current_datetime,
                    "current_weekday": current_weekday,
                    "current_hour": current_hour,
                    "current_minute": current_minute,
                    "current_timestamp": current_timestamp,
                    "ä»Šå¤©çš„å…·ä½“æ—¶é—´": current_datetime,
                    "å½“å‰æ—¶é—´": current_datetime,
                    "ä»Šå¤©": current_date,
                    "ç°åœ¨å‡ ç‚¹": current_time,
                    "æ˜ŸæœŸå‡ ": current_weekday,
                }

                formatted = custom_prompt
                for key, value in replacements.items():
                    try:
                        formatted = formatted.replace("{" + key + "}", str(value))
                    except Exception:
                        # å•ä¸ªæ›¿æ¢å¤±è´¥ä¸å½±å“æ•´ä½“
                        pass
                return formatted
        except Exception as e:
            print(f"âš ï¸ è·å–è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
        
        # é»˜è®¤æç¤ºè¯ï¼ˆå…œåº•æ–¹æ¡ˆï¼ŒåŠ å…¥å¯è§†åŒ–è§„èŒƒï¼‰
        now = datetime.now()
        current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
        return (
            "ä»Šå¤©æ˜¯{date}ï¼ˆ{weekday}ï¼‰ã€‚ä½ æ˜¯ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·ä½ ä½¿ç”¨å·¥å…·è§£ç­”ç”¨æˆ·çš„é—®é¢˜ï¼\n\n"
            "ã€å¯è§†åŒ–è¾“å‡ºï¼ˆechart + Mermaidï¼‰ã€‘\n"
            "- è‹¥éœ€å±•ç¤ºæ•°æ®å¯¹æ¯”/è¶‹åŠ¿/å æ¯”ï¼šè¾“å‡ºä¸€ä¸ªåˆæ³•çš„ ```echarts ä»£ç å—ï¼ˆæ ‡å‡†ECharts JSONï¼‰\n"
            "- è‹¥éœ€å±•ç¤ºæµç¨‹/ç»“æ„/æ—¶åºï¼šè¾“å‡º ```mermaid ä»£ç å—ï¼ˆflowchart/sequence/state/class/gantt/pieï¼‰\n"
            "- æ¯ä¸ªå›¾ä¸€ä¸ªä»£ç å—ï¼Œå°½é‡ä¸è¦å¤¹æ‚è§£é‡Šæ€§æ–‡å­—ã€‚"
            "ã€é‡è¦ã€‘ç»Ÿä¸€ä½¿ç”¨ ```echarts ä»£ç å—è¾“å‡º ECharts é…ç½®ï¼ˆoptionï¼‰ï¼Œä¸è¦è¾“å‡º ```chartjsã€‚\\n"
        ).format(date=current_date, weekday=current_weekday)

    def _get_stream_system_prompt(self) -> str:
        """ä¿æŒæ¥å£ä»¥å…¼å®¹æ—§è°ƒç”¨ï¼Œä½†å½“å‰ä¸å†ä½¿ç”¨æµå¼å›ç­”æç¤ºè¯ã€‚"""
        return ""

    # å·²ç§»è‡³ get_mcp_tools.py

    async def chat_stream(self, user_input, history: List[Dict[str, Any]] = None, session_id: Optional[str] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼æ¢æµ‹ + ç«‹å³ä¸­æ–­ï¼š
        - å…ˆç›´æ¥ astream å¼€æµï¼ŒçŸ­æš‚ç¼“å†²å¹¶æ£€æµ‹ function_call/tool_callï¼›
        - è‹¥æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼šç«‹å³ä¸­æ–­æœ¬æ¬¡æµå¼ï¼ˆä¸ä¸‹å‘ç¼“å†²ï¼‰ï¼Œæ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰ï¼Œå†™å› messages åè¿›å…¥ä¸‹ä¸€è½®ï¼›
        - è‹¥æœªæ£€æµ‹åˆ°å·¥å…·ï¼šå°†æœ¬æ¬¡æµä½œä¸ºæœ€ç»ˆå›ç­”ï¼Œå¼€å§‹æµå¼æ¨é€åˆ°ç»“æŸã€‚
        """
        try:
            if session_id:
                try:
                    self._current_session_id_ctx.set(session_id)
                except Exception:
                    pass
            try:
                preview = user_input if isinstance(user_input, str) else "[multimodal parts]"
                if isinstance(preview, str):
                    preview = preview[:50]
                print(f"ğŸ¤– å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {preview}...")
            except Exception:
                print("ğŸ¤– å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥ ...")
            yield {"type": "status", "content": "å¼€å§‹ç”Ÿæˆ..."}

            # ä¾æ®ä¼šè¯ä¸Šä¸‹æ–‡é€‰æ‹©æ¨¡å‹æ¡£ä½
            profile_id = None
            try:
                if session_id and self.session_contexts.get(session_id):
                    profile_id = self.session_contexts[session_id].get("model") or self.session_contexts[session_id].get("llm_profile")
            except Exception:
                profile_id = None

            # è‹¥é€‰æ‹©çš„æ˜¯ agent æ¡£ä½ï¼Œåˆ™èµ°å¤šæ™ºèƒ½ä½“ç¼–æ’æµç¨‹
            if profile_id and self.llm_profiles.get(profile_id, {}).get("kind") == "agent":
                agent_cfg = self.llm_profiles.get(profile_id, {})
                async for ev in self.agent_orchestrator.chat_stream(
                    user_input=user_input,
                    history=history,
                    session_id=session_id,
                    agent_cfg=agent_cfg,
                    session_contexts=self.session_contexts,
                ):
                    yield ev
                return

            # æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹(user-*)
            current_llm_tools = None
            try:
                if profile_id and str(profile_id).startswith("user-"):
                    user_map = self.session_contexts.get(session_id, {}).get("user_models") or {}
                    cfg = user_map.get(profile_id)
                    if cfg:
                        prev_key = os.getenv("OPENAI_API_KEY")
                        prev_base = os.getenv("OPENAI_BASE_URL")
                        try:
                            if cfg.get("api_key"):
                                os.environ["OPENAI_API_KEY"] = cfg["api_key"]
                            if cfg.get("base_url"):
                                os.environ["OPENAI_BASE_URL"] = cfg["base_url"]
                            _base = ChatOpenAI(
                                model=cfg.get("model", self.model_name),
                                temperature=cfg.get("temperature", self.temperature),
                                timeout=cfg.get("timeout", self.timeout),
                                max_retries=3,
                            )
                            current_llm_tools = _base.bind_tools(self.tools)
                        finally:
                            if prev_key is not None:
                                os.environ["OPENAI_API_KEY"] = prev_key
                            if prev_base is not None:
                                os.environ["OPENAI_BASE_URL"] = prev_base
            except Exception as __e:
                print(f"âš ï¸ ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹ç»‘å®šå·¥å…·å¤±è´¥: {__e}")
                current_llm_tools = None
            if current_llm_tools is None:
                llm_bundle = self._get_or_create_llm_instances(profile_id)
                current_llm_tools = llm_bundle.get("llm_tools", self.llm_tools)

            # 1) æ„å»ºå…±äº«æ¶ˆæ¯å†å²ï¼ˆä¸åŒ…å«ç³»ç»Ÿæç¤ºï¼Œä¾¿äºä¸¤å¥—ç³»ç»Ÿæç¤ºåˆ†åˆ«æ³¨å…¥ï¼‰
            # æ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦å·²çŸ¥ä¸æ”¯æŒå¤šæ¨¡æ€
            current_model_key = self._get_current_model_key(session_id)
            force_text_only = current_model_key in self._non_multimodal_models
            
            # ç²¾ç®€å†å²ï¼šä»…ä¿ç•™â€œé—®ç­”â€ä¸ºä¸»ï¼Œå·¥å…·ç»“æœåšç®€è¦æ³¨è®°ï¼Œå‡å°‘å™ªéŸ³
            shared_history = self.message_processor.build_shared_history(
                history, user_input, force_text_only, concise=True
            )

            max_rounds = 25
            round_index = 0
            # åˆå¹¶ä¸¤é˜¶æ®µè¾“å‡ºä¸ºåŒä¸€æ¡æ¶ˆæ¯ï¼šåœ¨æ•´ä¸ªä¼šè¯å›ç­”æœŸé—´ä»…å‘é€ä¸€æ¬¡ startï¼Œæœ€åä¸€æ¬¡æ€§ end
            combined_response_started = False
            # å·¥å…·è°ƒç”¨å¤±è´¥è®¡æ•°å™¨å’Œå…œåº•æœºåˆ¶
            consecutive_tool_failures = 0
            max_consecutive_failures = 3  # è¿ç»­å¤±è´¥3æ¬¡è§¦å‘å…œåº•
            tool_error_history = []  # è®°å½•é”™è¯¯å†å²
            while round_index < max_rounds:
                round_index += 1
                print(f"ğŸ§  ç¬¬ {round_index} è½®æ¨ç† (åŒå®ä¾‹ï¼šåˆ¤å®šå·¥å…· + çº¯æµå¼å›ç­”)...")

                # 2) ä½¿ç”¨å¸¦å·¥å…·å®ä¾‹åš"æµå¼åˆ¤å®š"ï¼š
                tools_messages = [{"role": "system", "content": self._get_tools_system_prompt()}] + shared_history
                tool_calls_check = None
                last_usage: Optional[Dict[str, Any]] = None
                buffered_chunks: List[str] = []
                content_preview = ""
                response_started = False
                multimodal_fallback_attempted = False
                
                try:
                    # æŠ‘åˆ¶MCPå®¢æˆ·ç«¯åœ¨åˆ¤å®šå·¥å…·æ—¶çš„SSEè§£æé”™è¯¯æ—¥å¿—
                    import logging
                    mcp_logger = logging.getLogger('mcp')
                    original_level = mcp_logger.level
                    mcp_logger.setLevel(logging.CRITICAL)
                    
                    try:
                        async for event in current_llm_tools.astream_events(tools_messages, version="v1"):
                            ev = event.get("event")
                            if ev == "on_chat_model_stream":
                                data = event.get("data", {})
                                chunk = data.get("chunk")
                                if chunk is None:
                                    continue
                                try:
                                    content_piece = getattr(chunk, 'content', None)
                                except Exception:
                                    content_piece = None
                                if content_piece:
                                    # ç«‹å³å‘å‰ç«¯æµå¼ä¸‹å‘ä½œä¸ºæœ€ç»ˆå›å¤ï¼ˆåˆå¹¶æ¨¡å¼ï¼šä»…é¦–æ¬¡å‘é€ startï¼‰
                                    if not combined_response_started:
                                        yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                                        combined_response_started = True
                                    response_started = True
                                    buffered_chunks.append(content_piece)
                                    try:
                                        print(f"ğŸ“¤ [åˆ¤å®šLLMæµ] {content_piece}")
                                    except Exception:
                                        pass
                                    yield {"type": "ai_response_chunk", "content": content_piece}
                            elif ev == "on_chat_model_end":
                                data = event.get("data", {})
                                output = data.get("output")
                                try:
                                    tool_calls_check = getattr(output, 'tool_calls', None)
                                except Exception:
                                    tool_calls_check = None
                                try:
                                    content_preview = getattr(output, 'content', None) or ""
                                except Exception:
                                    content_preview = ""
                                # æ•è·çœŸå®tokenç”¨é‡ï¼ˆè‹¥åº•å±‚è¿”å›ï¼‰
                                try:
                                    usage = getattr(output, 'usage_metadata', None)
                                    if not usage:
                                        meta = getattr(output, 'response_metadata', None) or {}
                                        # å…¼å®¹ä¸åŒSDKå­—æ®µ
                                        usage = meta.get('token_usage') or {
                                            k: meta.get(k) for k in ("input_tokens", "output_tokens", "total_tokens") if k in meta
                                        }
                                    if usage:
                                        # è§„èŒƒåŒ–ä¸ºdict
                                        if not isinstance(usage, dict):
                                            try:
                                                usage = dict(usage)
                                            except Exception:
                                                usage = {"raw": str(usage)}
                                        last_usage = {
                                            "input_tokens": usage.get("input_tokens"),
                                            "output_tokens": usage.get("output_tokens"),
                                            "total_tokens": usage.get("total_tokens") or (
                                                (usage.get("input_tokens") or 0) + (usage.get("output_tokens") or 0)
                                            )
                                        }
                                except Exception:
                                    last_usage = last_usage
                    finally:
                        mcp_logger.setLevel(original_level)
                except Exception as e:
                    error_msg = str(e)
                    print(f"âš ï¸ å·¥å…·åˆ¤å®š(æµå¼)å¤±è´¥ï¼š{error_msg}")
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¨¡æ€æ ¼å¼é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•é™çº§é‡è¯•
                    if not multimodal_fallback_attempted and self._is_multimodal_error(error_msg):
                        print(f"ğŸ”„ æ£€æµ‹åˆ°å¤šæ¨¡æ€æ ¼å¼é”™è¯¯ï¼Œå°è¯•é™çº§ä¸ºçº¯æ–‡æœ¬æ¨¡å¼...")
                        
                        # æ ‡è®°è¯¥æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€
                        self._non_multimodal_models.add(current_model_key)
                        
                        # å‘é€é™çº§æç¤º
                        if not combined_response_started:
                            yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                            combined_response_started = True
                        yield {"type": "ai_response_chunk", "content": "âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒå›¾ç‰‡è¯†åˆ«ï¼Œå·²è‡ªåŠ¨è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ¨¡å¼å¤„ç†ã€‚\n\n"}
                        
                        # è½¬æ¢æ¶ˆæ¯ä¸ºçº¯æ–‡æœ¬æ ¼å¼å¹¶é‡è¯•
                        text_only_messages = self._convert_multimodal_to_text(tools_messages)
                        multimodal_fallback_attempted = True
                        
                        try:
                            # é™çº§é‡è¯•æ—¶ä¹ŸæŠ‘åˆ¶MCPé”™è¯¯æ—¥å¿—
                            mcp_logger.setLevel(logging.CRITICAL)
                            try:
                                async for event in current_llm_tools.astream_events(text_only_messages, version="v1"):
                                    ev = event.get("event")
                                    if ev == "on_chat_model_stream":
                                        data = event.get("data", {})
                                        chunk = data.get("chunk")
                                        if chunk is None:
                                            continue
                                        try:
                                            content_piece = getattr(chunk, 'content', None)
                                        except Exception:
                                            content_piece = None
                                        if content_piece:
                                            response_started = True
                                            buffered_chunks.append(content_piece)
                                            try:
                                                print(f"ğŸ“¤ [é™çº§LLMæµ] {content_piece}")
                                            except Exception:
                                                pass
                                            yield {"type": "ai_response_chunk", "content": content_piece}
                                    elif ev == "on_chat_model_end":
                                        data = event.get("data", {})
                                        output = data.get("output")
                                        try:
                                            tool_calls_check = getattr(output, 'tool_calls', None)
                                        except Exception:
                                            tool_calls_check = None
                                        try:
                                            content_preview = getattr(output, 'content', None) or ""
                                        except Exception:
                                            content_preview = ""
                            finally:
                                mcp_logger.setLevel(original_level)
                        except Exception as fallback_e:
                            print(f"âŒ é™çº§é‡è¯•ä¹Ÿå¤±è´¥ï¼š{fallback_e}")
                            tool_calls_check = None
                            content_preview = ""
                    else:
                        tool_calls_check = None
                        content_preview = ""

                if tool_calls_check:
                    # åˆå¹¶æ¨¡å¼ï¼šä¸ç»“æŸæ¶ˆæ¯ï¼Œæ’å…¥åˆ†éš”åç»§ç»­æ‰§è¡Œå·¥å…·ï¼Œæœ€ç»ˆä¸€å¹¶ç»“æŸ
                    if response_started and buffered_chunks:
                        yield {"type": "ai_response_chunk", "content": "\n\n"}
                        buffered_chunks = []

                    tool_calls_to_run = tool_calls_check
                    yield {"type": "tool_plan", "content": f"AIå†³å®šè°ƒç”¨ {len(tool_calls_to_run)} ä¸ªå·¥å…·", "tool_count": len(tool_calls_to_run)}
                    # å†™å›assistantå¸¦tool_calls
                    try:
                        shared_history.append({
                            "role": "assistant",
                            "content": "",
                            "tool_calls": tool_calls_to_run
                        })
                    except Exception:
                        shared_history.append({"role": "assistant", "content": ""})

                    # æ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰
                    exit_to_stream = False
                    current_round_has_error = False
                    for i, tool_call in enumerate(tool_calls_to_run, 1):
                        if isinstance(tool_call, dict):
                            tool_id = tool_call.get('id') or f"call_{i}"
                            fn = tool_call.get('function') or {}
                            tool_name = fn.get('name') or tool_call.get('name') or ''
                            tool_args_raw = fn.get('arguments') or tool_call.get('args') or {}
                        else:
                            tool_id = getattr(tool_call, 'id', None) or f"call_{i}"
                            tool_name = getattr(tool_call, 'name', '') or ''
                            tool_args_raw = getattr(tool_call, 'args', {}) or {}

                        # è§£æå‚æ•°
                        if isinstance(tool_args_raw, str):
                            try:
                                parsed_args = json.loads(tool_args_raw) if tool_args_raw else {}
                            except Exception:
                                parsed_args = {"$raw": tool_args_raw}
                        elif isinstance(tool_args_raw, dict):
                            parsed_args = tool_args_raw
                        else:
                            parsed_args = {"$raw": str(tool_args_raw)}

                        yield {"type": "tool_start", "tool_id": tool_id, "tool_name": tool_name, "tool_args": parsed_args, "progress": f"{i}/{len(tool_calls_to_run)}"}

                        tool_execution_failed = False
                        try:
                            target_tool = None
                            for tool in self.tools:
                                if tool.name == tool_name:
                                    target_tool = tool
                                    break
                            if target_tool is None:
                                error_msg = f"å·¥å…· '{tool_name}' æœªæ‰¾åˆ°"
                                print(f"âŒ {error_msg}")
                                yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                                tool_result = f"é”™è¯¯: {error_msg}"
                                tool_execution_failed = True
                                current_round_has_error = True
                                tool_error_history.append({"tool": tool_name, "error": error_msg})
                            else:
                                # æŠ‘åˆ¶MCPå®¢æˆ·ç«¯åœ¨å·¥å…·è°ƒç”¨æ—¶çš„SSEè§£æé”™è¯¯æ—¥å¿—
                                import logging
                                mcp_logger = logging.getLogger('mcp')
                                original_level = mcp_logger.level
                                mcp_logger.setLevel(logging.CRITICAL)
                                
                                try:
                                    tool_result = await target_tool.ainvoke(parsed_args)
                                    yield {"type": "tool_end", "tool_id": tool_id, "tool_name": tool_name, "result": str(tool_result)}
                                    # å·¥å…·æ‰§è¡ŒæˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°å™¨
                                    consecutive_tool_failures = 0
                                    # ä¸å†æ”¯æŒé€€å‡ºå·¥å…·æ¨¡å¼
                                finally:
                                    mcp_logger.setLevel(original_level)
                        except Exception as e:
                            error_msg = f"å·¥å…·æ‰§è¡Œå‡ºé”™: {e}"
                            print(f"âŒ {error_msg}")
                            yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                            tool_result = f"é”™è¯¯: {error_msg}"
                            tool_execution_failed = True
                            current_round_has_error = True
                            tool_error_history.append({"tool": tool_name, "error": str(e)})

                        # å§‹ç»ˆè¿½åŠ  tool æ¶ˆæ¯ï¼Œæ»¡è¶³ OpenAI å‡½æ•°è°ƒç”¨åè®®è¦æ±‚
                        # å¯¹äºé€€å‡ºå·¥å…·æ¨¡å¼ï¼Œå†…å®¹ä¸ºç®€å•çŠ¶æ€ï¼Œä¸å½±å“åç»­å›ç­”è´¨é‡
                        shared_history.append({
                            "role": "tool",
                            "tool_call_id": tool_id,
                            "name": tool_name,
                            "content": str(tool_result)
                        })

                        if exit_to_stream:
                            break

                    # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°å™¨
                    if current_round_has_error:
                        consecutive_tool_failures += 1
                        print(f"âš ï¸ å·¥å…·è°ƒç”¨å¤±è´¥è®¡æ•°: {consecutive_tool_failures}/{max_consecutive_failures}")
                        
                        # è¾¾åˆ°é˜ˆå€¼,è§¦å‘å…œåº•å“åº”
                        if consecutive_tool_failures >= max_consecutive_failures:
                            print(f"ğŸ›Ÿ è§¦å‘å…œåº•æœºåˆ¶: è¿ç»­{consecutive_tool_failures}æ¬¡å·¥å…·è°ƒç”¨å¤±è´¥")
                            
                            # æ„å»ºé”™è¯¯æ‘˜è¦
                            error_summary = "\n".join([
                                f"- {err.get('tool', 'æœªçŸ¥å·¥å…·')}: {err.get('error', 'æœªçŸ¥é”™è¯¯')}"
                                for err in tool_error_history[-3:]  # åªæ˜¾ç¤ºæœ€è¿‘3ä¸ªé”™è¯¯
                            ])
                            
                            # ç”Ÿæˆå…œåº•å›å¤æç¤ºè¯
                            fallback_prompt = f"""å·¥å…·è°ƒç”¨å‡ºç°å¼‚å¸¸,è¯·æ ¹æ®å½“å‰å·²æœ‰ä¿¡æ¯ç»™ç”¨æˆ·ä¸€ä¸ªåˆç†çš„å›å¤ã€‚

é”™è¯¯æƒ…å†µ:
{error_summary}

è¯·å‘ŠçŸ¥ç”¨æˆ·:
1. ç³»ç»Ÿæš‚æ—¶æ— æ³•è·å–æ‰€éœ€æ•°æ®
2. æ ¹æ®å¯¹è¯å†å²,ç»™å‡ºåŸºäºå·²çŸ¥ä¿¡æ¯çš„å»ºè®®æˆ–æ›¿ä»£æ–¹æ¡ˆ
3. è¯­æ°”å‹å¥½ä¸“ä¸š,ä¸è¦è¿‡åˆ†é“æ­‰"""

                            # å°†å…œåº•æç¤ºåŠ å…¥å†å²
                            shared_history.append({
                                "role": "user",
                                "content": fallback_prompt
                            })
                            
                            # é€šçŸ¥å‰ç«¯è§¦å‘å…œåº•æœºåˆ¶
                            yield {
                                "type": "fallback_triggered",
                                "content": f"å·¥å…·è°ƒç”¨å¼‚å¸¸,æ­£åœ¨ç”Ÿæˆæ›¿ä»£å›å¤...",
                                "error_count": consecutive_tool_failures
                            }
                            
                            # å¼ºåˆ¶è¿›å…¥ä¸‹ä¸€è½®,è®©æ¨¡å‹åŸºäºå…œåº•æç¤ºç”Ÿæˆå›å¤
                            # é‡ç½®è®¡æ•°å™¨,é¿å…å†æ¬¡è§¦å‘
                            consecutive_tool_failures = 0
                            tool_error_history = []
                            continue

                    if exit_to_stream:
                        # ä¸å†æ”¯æŒæå‰å¼ºåˆ¶åˆ‡æµå¼ï¼ŒæŒ‰åŸé€»è¾‘ç»§ç»­ä¸‹ä¸€è½®
                        pass
                    else:
                        # å·¥å…·åç»§ç»­ä¸‹ä¸€è½®
                        continue

                # 3) æ— å·¥å…·ï¼šåˆå¹¶æ¨¡å¼
                # è‹¥å…ˆå‰å·²ç»æµå¼è¾“å‡ºè¿‡ç‰‡æ®µï¼Œåˆ™æ­¤å¤„ä¸å†æŠŠæ‰€æœ‰ç‰‡æ®µå†å‘ä¸€æ¬¡ï¼Œåªå‘é€ç»“æŸæ ‡è®°ï¼›
                # è‹¥æ­¤å‰å°šæœªå¼€å§‹ï¼ˆæ— æµå¼ç‰‡æ®µï¼‰ï¼Œåˆ™ä¸€æ¬¡æ€§å‘é€æœ€ç»ˆæ–‡æœ¬å†ç»“æŸã€‚
                final_text = "".join(buffered_chunks) if buffered_chunks else (content_preview or "")
                if combined_response_started:
                    # å·²ç»å¼€å§‹è¿‡ï¼Œé¿å…é‡å¤å†…å®¹
                    yield {"type": "ai_response_end", "content": ""}
                else:
                    yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                    combined_response_started = True
                    if final_text:
                        try:
                            print(f"ğŸ“¤ [æœ€ç»ˆå›å¤æµ] {final_text}")
                        except Exception:
                            pass
                        yield {"type": "ai_response_chunk", "content": final_text}
                    yield {"type": "ai_response_end", "content": ""}
                # åœ¨ç»“æŸåè¡¥å‘tokenç”¨é‡ï¼ˆè‹¥å¯ç”¨ï¼‰
                if last_usage:
                    try:
                        yield {"type": "token_usage", **last_usage}
                    except Exception:
                        pass
                return

            # è½®æ¬¡è€—å°½ï¼šç›´æ¥è¿”å›æç¤ºä¿¡æ¯
            print(f"âš ï¸ è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°({max_rounds})ï¼Œç›´æ¥è¿”å›æç¤ºä¿¡æ¯")
            final_text = "å·²è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°ï¼Œè¯·ç¼©å°é—®é¢˜èŒƒå›´æˆ–ç¨åé‡è¯•ã€‚"
            yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
            yield {"type": "ai_response_chunk", "content": final_text}
            yield {"type": "ai_response_end", "content": final_text}
            return
        except Exception as e:
            import traceback
            print(f"âŒ chat_stream å¼‚å¸¸: {e}")
            print("ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            yield {"type": "error", "content": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"}

    def get_tools_info(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ä¿¡æ¯åˆ—è¡¨ï¼ŒæŒ‰MCPæœåŠ¡å™¨åˆ†ç»„"""
        return self.tools_manager.get_tools_info()

    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            await self.tools_manager.close()
        except:
            pass
